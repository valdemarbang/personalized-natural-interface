{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5246d8",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb977b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shutil, contextlib, os, time, re, subprocess\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "from chatterbox.models.voice_encoder import VoiceEncoder\n",
    "import torchaudio as ta\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "from chatterbox.mtl_tts import ChatterboxMultilingualTTS\n",
    "from chatterbox.mtl_tts import Conditionals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b3cee",
   "metadata": {},
   "source": [
    "# Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2db374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "has_cuda = torch.cuda.is_available()\n",
    "has_mps = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    fp16 = False\n",
    "    no_cuda = True\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    fp16 = True\n",
    "    no_cuda = False\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    fp16 = False\n",
    "    no_cuda = True\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97731334",
   "metadata": {},
   "source": [
    "## Download Model into Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996152f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 10 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 576.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model saved to: /nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/models/base/chatterbox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Choose the exact base you fine-tuned from\n",
    "REPO_ID = \"ResembleAI/chatterbox\"\n",
    "\n",
    "# ---- Project dirs (â€¦/models/{base,checkpoints,hf_cache})\n",
    "parent_dir = Path(os.getcwd()).parent\n",
    "models_folder = parent_dir / \"models\"\n",
    "base_folder = models_folder / \"base\" / REPO_ID.split(\"/\")[-1]\n",
    "checkpoints_folder = models_folder / \"checkpoints\"\n",
    "cache_folder  = models_folder / \"hf_cache\"\n",
    "\n",
    "for p in (base_folder, checkpoints_folder, cache_folder):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Allow filename variants seen across releases\n",
    "allow = [\n",
    "    \"ve.pt\", \"ve.safetensors\",\n",
    "    \"s3gen.pt\",\n",
    "    \"conds.pt\",\n",
    "    \"Cangjie5_TC.json\",\n",
    "    # tokenizer variants\n",
    "    \"mtl_tokenizer.json\", \"tokenizer.json\", \"grapheme_mtl_merged_expanded_v1.json\",\n",
    "    # T3 variants\n",
    "    \"t3_23lang.safetensors\", \"t3_mtl23ls_v2.safetensors\",\n",
    "]\n",
    "\n",
    "ckpt_dir = Path(snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"model\",\n",
    "    revision=\"main\",\n",
    "    allow_patterns=allow,\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    cache_dir=str(cache_folder),\n",
    "    local_dir=str(base_folder),           # <- save into models/base/<repo>\n",
    "    local_dir_use_symlinks=False          # <- write real files\n",
    "))\n",
    "\n",
    "print(\"Base model saved to:\", ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb8e00",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7c73cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python at:\n",
      "/nobackup/joeri765/venvs/pni-py310/bin/python3\n",
      "Running preprocessing: python3 ../scripts/preprocess_data.py --metadata_file ../data/david/metadata.txt --local_model_dir /nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/models/base/chatterbox --output_path ../data/david/precomputed_t3_features.pt --language_id sv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/perth/perth_net/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata file: ../data/david/metadata.txt\n",
      "Local model dir: /nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/models/base/chatterbox\n",
      "Output path: ../data/david/precomputed_t3_features.pt\n",
      "Language ID: sv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded PerthNet (Implicit) at step 250,000\n",
      "Audio file does not exist: /nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/data/david/soundfiles_david_sentence05.wav\n",
      "Loaded 88 lines from metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing T3 features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:27<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 88 examples to ../data/david/precomputed_t3_features.pt\n",
      "âœ… Preprocessing done, saved to ../data/david/precomputed_t3_features.pt\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"../models/checkpoints/chatterbox_finetuned_swedish\"\n",
    "local_model_dir = str(ckpt_dir)\n",
    "metadata_file = \"../data/david/metadata.txt\"\n",
    "dataset_dir = \"../data/david\"\n",
    "dataloader_num_workers = 0\n",
    "precomputed_output = \"../data/david/precomputed_t3_features.pt\"\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Using python at:\")\n",
    "subprocess.run([\"which\", \"python3\"], check=True)\n",
    "\n",
    "# environment (reuse what you already have if you want)\n",
    "env = os.environ.copy()\n",
    "\n",
    "# 1) RUN PREPROCESSING SCRIPT ONCE\n",
    "preprocess_cmd = [\n",
    "    \"python3\", \"../scripts/preprocess_data.py\",\n",
    "    \"--metadata_file\", metadata_file,\n",
    "    \"--local_model_dir\", local_model_dir,\n",
    "    \"--output_path\", precomputed_output,\n",
    "    \"--language_id\", \"sv\",\n",
    "]\n",
    "\n",
    "print(\"Running preprocessing:\", \" \".join(preprocess_cmd))\n",
    "subprocess.run(preprocess_cmd, env=env, check=True)\n",
    "\n",
    "print(\"âœ… Preprocessing done, saved to\", precomputed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0912ede",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981acbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python at:\n",
      "Running: python3 ../scripts/finetune_mtl_precomputed.py --output_dir ../models/checkpoints/chatterbox_finetuned_swedish --local_model_dir ../models/base/chatterbox --precomputed_path ../data/david/precomputed_t3_features.pt --eval_split_size 0.1 --num_train_epochs 20 --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --learning_rate 1e-5 --warmup_steps 200 --weight_decay 0.01 --max_grad_norm 1.0 --logging_steps 10 --evaluation_strategy steps --eval_steps 500 --save_strategy steps --save_steps 500 --save_total_limit 5 --report_to tensorboard --do_train --do_eval --dataloader_pin_memory False --eval_on_start True --label_names labels_speech --language_id sv --dataloader_num_workers 0 --seed 42 --fp16 --gradient_checkpointing True --lr_scheduler_type cosine --early_stopping_patience 3 --load_best_model_at_end True --metric_for_best_model eval_loss --greater_is_better False\n",
      "/nobackup/joeri765/venvs/pni-py310/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/perth/perth_net/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "11/24/2025 10:58:09 - INFO - __main__ - Training/evaluation parameters CustomTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=False,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "early_stopping_patience=3,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=True,\n",
      "eval_steps=500,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=['labels_speech'],\n",
      "label_smoothing_factor=0.0,\n",
      "language_id=sv,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/checkpoints/chatterbox_finetuned_swedish/runs/Nov24_10-58-09_asgard1-415.ad.liu.se,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=20.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=../models/checkpoints/chatterbox_finetuned_swedish,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/checkpoints/chatterbox_finetuned_swedish,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=200,\n",
      "weight_decay=0.01,\n",
      ")\n",
      "11/24/2025 10:58:09 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path=None, local_model_dir='../models/base/chatterbox', cache_dir=None, freeze_voice_encoder=True, freeze_s3gen=True)\n",
      "11/24/2025 10:58:09 - INFO - __main__ - Data parameters PrecomputedDataArguments(precomputed_path='../data/david/precomputed_t3_features.pt', eval_split_size=0.1)\n",
      "11/24/2025 10:58:09 - INFO - __main__ - Using language_id: sv\n",
      "11/24/2025 10:58:09 - INFO - __main__ - Loading base model from local directory: ../models/base/chatterbox\n",
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "11/24/2025 10:58:13 - INFO - root - input frame rate=25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded PerthNet (Implicit) at step 250,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/24/2025 10:58:14 - INFO - __main__ - Voice Encoder frozen.\n",
      "11/24/2025 10:58:14 - INFO - __main__ - S3Gen model frozen.\n",
      "11/24/2025 10:58:14 - INFO - __main__ - T3 model set to trainable.\n",
      "11/24/2025 10:58:14 - INFO - __main__ - Loading precomputed features from: ../data/david/precomputed_t3_features.pt\n",
      "11/24/2025 10:58:14 - INFO - __main__ - Total precomputed examples: 88\n",
      "11/24/2025 10:58:14 - INFO - __main__ - Train examples: 80 | Eval examples: 8\n",
      "11/24/2025 10:58:15 - INFO - __main__ - *** Training Multilingual T3 model on precomputed data ***\n",
      "Traceback (most recent call last):\n",
      "  File \"/nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/notebooks/../scripts/finetune_mtl_precomputed.py\", line 268, in <module>\n",
      "    main()\n",
      "  File \"/nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/notebooks/../scripts/finetune_mtl_precomputed.py\", line 203, in main\n",
      "    train_result = trainer.train(\n",
      "  File \"/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/transformers/trainer.py\", line 2123, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/transformers/trainer.py\", line 2253, in _inner_training_loop\n",
      "    self.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=args.gradient_checkpointing_kwargs)\n",
      "  File \"/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1928, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'T3ForFineTuning' object has no attribute 'gradient_checkpointing_enable'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python3', '../scripts/finetune_mtl_precomputed.py', '--output_dir', '../models/checkpoints/chatterbox_finetuned_swedish', '--local_model_dir', '../models/base/chatterbox', '--precomputed_path', '../data/david/precomputed_t3_features.pt', '--eval_split_size', '0.1', '--num_train_epochs', '20', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '4', '--learning_rate', '1e-5', '--warmup_steps', '200', '--weight_decay', '0.01', '--max_grad_norm', '1.0', '--logging_steps', '10', '--evaluation_strategy', 'steps', '--eval_steps', '500', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '5', '--report_to', 'tensorboard', '--do_train', '--do_eval', '--dataloader_pin_memory', 'False', '--eval_on_start', 'True', '--label_names', 'labels_speech', '--language_id', 'sv', '--dataloader_num_workers', '0', '--seed', '42', '--fp16', '--gradient_checkpointing', 'True', '--lr_scheduler_type', 'cosine', '--early_stopping_patience', '3', '--load_best_model_at_end', 'True', '--metric_for_best_model', 'eval_loss', '--greater_is_better', 'False']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     cmd\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--no_cuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cmd))\n\u001b[0;32m---> 63\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    527\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['python3', '../scripts/finetune_mtl_precomputed.py', '--output_dir', '../models/checkpoints/chatterbox_finetuned_swedish', '--local_model_dir', '../models/base/chatterbox', '--precomputed_path', '../data/david/precomputed_t3_features.pt', '--eval_split_size', '0.1', '--num_train_epochs', '20', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '4', '--learning_rate', '1e-5', '--warmup_steps', '200', '--weight_decay', '0.01', '--max_grad_norm', '1.0', '--logging_steps', '10', '--evaluation_strategy', 'steps', '--eval_steps', '500', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '5', '--report_to', 'tensorboard', '--do_train', '--do_eval', '--dataloader_pin_memory', 'False', '--eval_on_start', 'True', '--label_names', 'labels_speech', '--language_id', 'sv', '--dataloader_num_workers', '0', '--seed', '42', '--fp16', '--gradient_checkpointing', 'True', '--lr_scheduler_type', 'cosine', '--early_stopping_patience', '3', '--load_best_model_at_end', 'True', '--metric_for_best_model', 'eval_loss', '--greater_is_better', 'False']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path(\"../models/base/chatterbox\")  # same dir you used for preprocessing\n",
    "output_dir = \"../models/checkpoints/chatterbox_finetuned_swedish\"\n",
    "precomputed_file = \"../data/david/precomputed_t3_features.pt\"\n",
    "dataloader_num_workers = 0\n",
    "\n",
    "# Device helpers (if you already have has_mps / no_cuda defined, reuse them)\n",
    "has_mps = torch.backends.mps.is_available() if \"torch\" in globals() else False\n",
    "no_cuda = False  # set True if you want to force CPU\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Using python at:\")\n",
    "subprocess.run([\"which\", \"python3\"], check=True)\n",
    "\n",
    "cmd = [\n",
    "    \"python3\", \"../scripts/finetune_mtl_precomputed.py\",\n",
    "    \"--output_dir\", output_dir,\n",
    "    \"--local_model_dir\", str(ckpt_dir),\n",
    "    \"--precomputed_path\", precomputed_file,\n",
    "    \"--eval_split_size\", \"0.1\",\n",
    "    \"--num_train_epochs\", \"20\",\n",
    "    \"--per_device_train_batch_size\", \"2\",\n",
    "    \"--gradient_accumulation_steps\", \"4\",\n",
    "    \"--learning_rate\", \"1e-5\",\n",
    "    \"--warmup_steps\", \"200\",\n",
    "    \"--weight_decay\", \"0.01\",\n",
    "    \"--max_grad_norm\", \"1.0\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "    \"--evaluation_strategy\", \"steps\",\n",
    "    \"--eval_steps\", \"500\",\n",
    "    \"--save_strategy\", \"steps\",\n",
    "    \"--save_steps\", \"500\",\n",
    "    \"--save_total_limit\", \"5\",\n",
    "    \"--report_to\", \"tensorboard\",\n",
    "    \"--do_train\",\n",
    "    \"--do_eval\",\n",
    "    \"--dataloader_pin_memory\", \"False\",\n",
    "    \"--eval_on_start\", \"True\",\n",
    "    \"--label_names\", \"labels_speech\",\n",
    "    \"--language_id\", \"sv\",\n",
    "    \"--dataloader_num_workers\", str(dataloader_num_workers),\n",
    "    \"--seed\", \"42\",\n",
    "    \"--fp16\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--early_stopping_patience\", \"3\",\n",
    "    \"--load_best_model_at_end\", \"True\",\n",
    "    \"--metric_for_best_model\", \"eval_loss\",\n",
    "    \"--greater_is_better\", \"False\",\n",
    "]\n",
    "\n",
    "env = os.environ.copy()\n",
    "if has_mps:\n",
    "    env[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "if no_cuda:\n",
    "    cmd.append(\"--no_cuda\")\n",
    "\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "subprocess.run(cmd, env=env, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769f0b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python at:\n",
      "/nobackup/joeri765/venvs/pni-py310/bin/python3\n",
      "Running: python3 ../scripts/finetune_mtl.py --output_dir ../models/checkpoints/chatterbox_finetuned_swedish --local_model_dir /nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/models/base/chatterbox --metadata_file ../data/david/metadata.txt --dataset_dir ../data/david --train_split_name train --eval_split_size 0.1 --num_train_epochs 10 --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --learning_rate 1e-5 --warmup_steps 200 --weight_decay 0.01 --max_grad_norm 1.0 --logging_steps 10 --evaluation_strategy epoch --save_strategy epoch --save_total_limit 5 --report_to tensorboard --do_train --do_eval --dataloader_pin_memory False --eval_on_start True --label_names labels_speech --text_column_name text_scribe --language_id sv --dataloader_num_workers 0 --seed 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/perth/perth_net/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "11/16/2025 11:00:25 - INFO - __main__ - Training/evaluation parameters CustomTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=False,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "early_stopping_patience=None,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=['labels_speech'],\n",
      "label_smoothing_factor=0.0,\n",
      "language_id=sv,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/checkpoints/chatterbox_finetuned_swedish/runs/Nov16_11-00-25_egypten1-409.ad.liu.se,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=../models/checkpoints/chatterbox_finetuned_swedish,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/checkpoints/chatterbox_finetuned_swedish,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=200,\n",
      "weight_decay=0.01,\n",
      ")\n",
      "11/16/2025 11:00:25 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path=None, local_model_dir='/nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/models/base/chatterbox', cache_dir=None, freeze_voice_encoder=True, freeze_s3gen=True)\n",
      "11/16/2025 11:00:25 - INFO - __main__ - Data parameters DataArguments(dataset_dir='../data/david', metadata_file='../data/david/metadata.txt', dataset_name=None, dataset_config_name=None, train_split_name='train', eval_split_name='validation', text_column_name='text_scribe', audio_column_name='audio', max_text_len=256, max_speech_len=800, audio_prompt_duration_s=3.0, eval_split_size=0.1, preprocessing_num_workers=None, ignore_verifications=False)\n",
      "11/16/2025 11:00:25 - INFO - __main__ - Using language_id: sv\n",
      "11/16/2025 11:00:25 - INFO - __main__ - Loading ChatterboxMultilingualTTS model...\n",
      "11/16/2025 11:00:25 - INFO - __main__ - Loading model from local directory: /nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/models/base/chatterbox\n",
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "11/16/2025 11:00:48 - INFO - root - input frame rate=25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded PerthNet (Implicit) at step 250,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/16/2025 11:01:03 - INFO - __main__ - Voice Encoder frozen.\n",
      "11/16/2025 11:01:03 - INFO - __main__ - S3Gen model frozen.\n",
      "11/16/2025 11:01:03 - INFO - __main__ - T3 model set to trainable.\n",
      "11/16/2025 11:01:03 - INFO - __main__ - Loading and processing dataset...\n",
      "11/16/2025 11:01:03 - WARNING - __main__ - Audio file not found: ../data/david/soundfiles_david_sentence05.wav (line 5). Skipping.\n",
      "11/16/2025 11:01:04 - INFO - __main__ - *** Training Multilingual T3 model ***\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "                                       6it/s]\u001b[A\n",
      "  0%|          | 0/100 [00:08<?, ?it/s]      \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.46it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 8.891, 'eval_samples_per_second': 1.012, 'eval_steps_per_second': 0.225, 'epoch': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 10/100 [00:41<04:40,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.536, 'grad_norm': 82.55428314208984, 'learning_rate': 5.000000000000001e-07, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 10%|â–ˆ         | 10/100 [00:44<04:40,  3.12s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 38.74it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.6835, 'eval_samples_per_second': 3.354, 'eval_steps_per_second': 0.745, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      " 20%|â–ˆâ–ˆ        | 20/100 [02:16<05:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.9127, 'grad_norm': 88.62390899658203, 'learning_rate': 1.0000000000000002e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 20%|â–ˆâ–ˆ        | 20/100 [02:18<05:00,  3.76s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 34.47it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.8635, 'eval_samples_per_second': 3.143, 'eval_steps_per_second': 0.698, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [03:51<04:30,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.3262, 'grad_norm': 105.07868194580078, 'learning_rate': 1.5e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [03:54<04:30,  3.86s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 30.44it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.9363, 'eval_samples_per_second': 3.065, 'eval_steps_per_second': 0.681, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [05:29<03:49,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.384, 'grad_norm': 63.533447265625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [05:31<03:49,  3.82s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 31.23it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.8818, 'eval_samples_per_second': 3.123, 'eval_steps_per_second': 0.694, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [07:04<03:08,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.7571, 'grad_norm': 46.5969123840332, 'learning_rate': 2.5e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [07:07<03:08,  3.77s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 33.35it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.8949, 'eval_samples_per_second': 3.109, 'eval_steps_per_second': 0.691, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [08:40<02:31,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.8698, 'grad_norm': 51.522056579589844, 'learning_rate': 3e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [08:43<02:31,  3.78s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 36.68it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.8614, 'eval_samples_per_second': 3.145, 'eval_steps_per_second': 0.699, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [10:18<01:54,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.6737, 'grad_norm': 50.73470687866211, 'learning_rate': 3.5e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [10:21<01:54,  3.83s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 37.28it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.8545, 'eval_samples_per_second': 3.153, 'eval_steps_per_second': 0.701, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [11:55<01:12,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.9493, 'grad_norm': 66.8690185546875, 'learning_rate': 4.000000000000001e-06, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [11:58<01:12,  3.62s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 27.87it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.8519, 'eval_samples_per_second': 3.156, 'eval_steps_per_second': 0.701, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [13:31<00:37,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6451, 'grad_norm': 65.4202651977539, 'learning_rate': 4.5e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [13:34<00:37,  3.76s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 28.58it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.9203, 'eval_samples_per_second': 3.082, 'eval_steps_per_second': 0.685, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [15:06<00:00,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5935, 'grad_norm': 84.77827453613281, 'learning_rate': 5e-06, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "\n",
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [16:11<00:00,  3.71s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 26.90it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2.6493, 'eval_samples_per_second': 3.397, 'eval_steps_per_second': 0.755, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [17:12<00:00, 10.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1032.8254, 'train_samples_per_second': 0.765, 'train_steps_per_second': 0.097, 'train_loss': 10.664745750427246, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/16/2025 11:18:38 - INFO - __main__ - Saving finetuned Multilingual T3 model weights for ChatterboxMultilingualTTS...\n",
      "11/16/2025 11:18:59 - INFO - __main__ - Finetuned Multilingual T3 model weights saved to ../models/checkpoints/chatterbox_finetuned_swedish/t3_mtl23ls_v2.safetensors\n",
      "11/16/2025 11:19:10 - INFO - __main__ - Full model components structured in ../models/checkpoints/chatterbox_finetuned_swedish\n",
      "11/16/2025 11:19:10 - INFO - __main__ - *** Evaluating Multilingual T3 model ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =        0GF\n",
      "  train_loss               =    10.6647\n",
      "  train_runtime            = 0:17:12.82\n",
      "  train_samples_per_second =      0.765\n",
      "  train_steps_per_second   =      0.097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 36.55it/s]\n",
      "11/16/2025 11:19:13 - INFO - __main__ - Multilingual finetuning script finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =        nan\n",
      "  eval_runtime            = 0:00:02.69\n",
      "  eval_samples_per_second =      3.344\n",
      "  eval_steps_per_second   =      0.743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python3', '../scripts/finetune_mtl.py', '--output_dir', '../models/checkpoints/chatterbox_finetuned_swedish', '--local_model_dir', '/nobackup/joeri765/projects/personalized-natural-interface/TTS/chatterbox/models/base/chatterbox', '--metadata_file', '../data/david/metadata.txt', '--dataset_dir', '../data/david', '--train_split_name', 'train', '--eval_split_size', '0.1', '--num_train_epochs', '10', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '4', '--learning_rate', '1e-5', '--warmup_steps', '200', '--weight_decay', '0.01', '--max_grad_norm', '1.0', '--logging_steps', '10', '--evaluation_strategy', 'epoch', '--save_strategy', 'epoch', '--save_total_limit', '5', '--report_to', 'tensorboard', '--do_train', '--do_eval', '--dataloader_pin_memory', 'False', '--eval_on_start', 'True', '--label_names', 'labels_speech', '--text_column_name', 'text_scribe', '--language_id', 'sv', '--dataloader_num_workers', '0', '--seed', '42'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"../models/checkpoints/chatterbox_finetuned_swedish\"\n",
    "local_model_dir = str(ckpt_dir)\n",
    "metadata_file = \"../data/david/metadata.txt\"\n",
    "dataset_dir = \"../data/david\"\n",
    "dataloader_num_workers = 0\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Using python at:\")\n",
    "subprocess.run([\"which\", \"python3\"], check=True)\n",
    "\n",
    "cmd = [\n",
    "    \"python3\", \"../scripts/finetune_mtl.py\",\n",
    "    \"--output_dir\", output_dir,\n",
    "    \"--local_model_dir\", local_model_dir,\n",
    "    \"--metadata_file\", metadata_file,\n",
    "    \"--dataset_dir\", dataset_dir,\n",
    "    \"--train_split_name\", \"train\",\n",
    "    \"--eval_split_size\", \"0.1\",\n",
    "    \"--num_train_epochs\", \"10\",\n",
    "    \"--per_device_train_batch_size\", \"2\",\n",
    "    \"--gradient_accumulation_steps\", \"4\",\n",
    "    \"--learning_rate\", \"1e-5\",\n",
    "    \"--warmup_steps\", \"200\",\n",
    "    \"--weight_decay\", \"0.01\",\n",
    "    \"--max_grad_norm\", \"1.0\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "    \"--evaluation_strategy\", \"epoch\",\n",
    "    \"--save_strategy\", \"epoch\",\n",
    "    \"--save_total_limit\", \"5\",\n",
    "    \"--report_to\", \"tensorboard\",\n",
    "    \"--do_train\",\n",
    "    \"--do_eval\",\n",
    "    \"--dataloader_pin_memory\", \"False\",\n",
    "    \"--eval_on_start\", \"True\",\n",
    "    \"--label_names\", \"labels_speech\",\n",
    "    \"--text_column_name\", \"text_scribe\",\n",
    "    \"--language_id\", \"sv\",\n",
    "    \"--dataloader_num_workers\", str(dataloader_num_workers),\n",
    "    \"--seed\", \"42\",\n",
    "]\n",
    "\n",
    "# add flags from device logic\n",
    "env = os.environ.copy()\n",
    "if has_mps:\n",
    "    env[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "if no_cuda:\n",
    "    cmd.append(\"--no_cuda\")\n",
    "\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "subprocess.run(cmd, env=env, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ef6ab",
   "metadata": {},
   "source": [
    "## Create Chatterbox-MTL model from finetuned checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb6977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine base + latest HF checkpoint into a runnable folder, then load.\n",
    "@contextlib.contextmanager\n",
    "def force_cpu_load():\n",
    "    from torch.serialization import load as _orig_load\n",
    "    def _cpu_load(*args, **kwargs):\n",
    "        kwargs[\"map_location\"] = torch.device(\"cpu\")\n",
    "        return _orig_load(*args, **kwargs)\n",
    "    old = torch.load\n",
    "    torch.load = _cpu_load\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        torch.load = old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c05b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FT checkpoint: ../models/checkpoints/chatterbox_finetuned_swedish/checkpoint-100/model.safetensors\n",
      "[OK] Wrote finetuned T3 to: ../models/checkpoints/chatterbox_finetuned_swedish/merged_for_infer/t3_mtl23ls_v2.safetensors and ../models/checkpoints/chatterbox_finetuned_swedish/merged_for_infer/t3_23lang.safetensors\n"
     ]
    }
   ],
   "source": [
    "# ---- Paths ----\n",
    "ft_root = Path(\"../models/checkpoints/chatterbox_finetuned_swedish\")\n",
    "\n",
    "# ---- Latest HF checkpoint ----\n",
    "ckpt_dirs = sorted(\n",
    "    [p for p in ft_root.iterdir() if p.is_dir() and p.name.startswith(\"checkpoint-\")],\n",
    "    key=lambda p: int(p.name.split(\"-\")[1])\n",
    ")\n",
    "\n",
    "ft_ckpt = ckpt_dirs[-1] if ckpt_dirs else ft_root\n",
    "src_model = ft_ckpt / \"model.safetensors\"\n",
    "assert src_model.exists(), f\"Missing checkpoint file: {src_model}\"\n",
    "print(\"Using FT checkpoint:\", src_model)\n",
    "\n",
    "# ---- Build merged_for_infer from BASE assets ----\n",
    "merged_dir = ft_root / \"merged_for_infer\"\n",
    "if merged_dir.exists():\n",
    "    shutil.rmtree(merged_dir)\n",
    "merged_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Copy base assets ----\n",
    "copies = [\n",
    "    (\"ve.pt\", \"ve.pt\"),\n",
    "    (\"s3gen.pt\", \"s3gen.pt\"),\n",
    "    (\"conds.pt\", \"conds.pt\"),\n",
    "    (\"Cangjie5_TC.json\", \"Cangjie5_TC.json\"),\n",
    "    (\"mtl_tokenizer.json\", \"mtl_tokenizer.json\"),\n",
    "    (\"tokenizer.json\", \"mtl_tokenizer.json\"),\n",
    "    (\"grapheme_mtl_merged_expanded_v1.json\", \"mtl_tokenizer.json\"),\n",
    "    (\"t3_mtl23ls_v2.safetensors\", \"t3_mtl23ls_v2.safetensors\"),\n",
    "    (\"t3_23lang.safetensors\", \"t3_23lang.safetensors\"),\n",
    "]\n",
    "for src_name, dst_name in copies:\n",
    "    src = base_folder / src_name\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, merged_dir / dst_name)\n",
    "\n",
    "# ---- Require tokenizer and at least one T3 base file to be present ----\n",
    "assert (merged_dir / \"mtl_tokenizer.json\").exists(), \\\n",
    "    f\"Tokenizer not found in base_dir: {base_folder}\"\n",
    "t3_primary = None\n",
    "for cand in (\"t3_mtl23ls_v2.safetensors\", \"t3_23lang.safetensors\"):\n",
    "    if (merged_dir / cand).exists():\n",
    "        t3_primary = cand\n",
    "        break\n",
    "assert t3_primary is not None, \\\n",
    "    f\"No base T3 file found in {base_folder} (looked for t3_mtl23ls_v2.safetensors / t3_23lang.safetensors).\"\n",
    "\n",
    "# ---- Convert FT checkpoint â†’ T3 weights and overwrite base T3\n",
    "sd = load_file(str(src_model))  # dict[str, Tensor]\n",
    "t3_sd = {k.split(\"t3.\", 1)[1]: v for k, v in sd.items() if k.startswith(\"t3.\")} \\\n",
    "        if any(k.startswith(\"t3.\") for k in sd) else dict(sd)\n",
    "assert any(k.startswith((\"tfmr.layers.\", \"tfmr.embed_tokens\")) for k in t3_sd), \\\n",
    "       \"Converted state dict doesn't look like T3 weights.\"\n",
    "\n",
    "# ---- Write to the base T3 filename and also to the other alias for compatibility ----\n",
    "save_file(t3_sd, str(merged_dir / t3_primary))\n",
    "alias = \"t3_23lang.safetensors\" if t3_primary == \"t3_mtl23ls_v2.safetensors\" else \"t3_mtl23ls_v2.safetensors\"\n",
    "save_file(t3_sd, str(merged_dir / alias))\n",
    "\n",
    "print(\"[OK] Wrote finetuned T3 to:\", (merged_dir / t3_primary), \"and\", (merged_dir / alias))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2715ec73",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "430891d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chatterbox_model(model_folder: str, device: str | None = None):\n",
    "    \"\"\"\n",
    "    Load a ChatterboxMultilingualTTS model from a folder.\n",
    "    Uses force_cpu_load to be safe across CUDA/MPS/CPU.\n",
    "    \"\"\"\n",
    "    model_folder = Path(model_folder)\n",
    "\n",
    "    # If device not given, fall back to global `device` if you have one\n",
    "    if device is None:\n",
    "        device = globals().get(\"device\", \"cpu\")\n",
    "\n",
    "    print(f\"Loading model from {model_folder} on device={device} (deserializing on CPU)â€¦\")\n",
    "\n",
    "    with force_cpu_load():\n",
    "        tts = ChatterboxMultilingualTTS.from_local(model_folder, device)\n",
    "\n",
    "    if device in (\"cuda\", \"mps\"):\n",
    "        tts.t3.to(device)\n",
    "        tts.s3gen.to(device)\n",
    "        tts.ve.to(device)\n",
    "        tts.device = device\n",
    "\n",
    "    print(\"Model loaded.\")\n",
    "    return tts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a83e96",
   "metadata": {},
   "source": [
    "### Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3160637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../models/base/chatterbox on device=cpu (deserializing on CPU)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded PerthNet (Implicit) at step 250,000\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "base_model = load_chatterbox_model(\n",
    "    \"../models/base/chatterbox\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4574b0",
   "metadata": {},
   "source": [
    "### Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c961207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../models/checkpoints/chatterbox_finetuned_swedish/merged_for_infer on device=cpu (deserializing on CPU)â€¦\n",
      "loaded PerthNet (Implicit) at step 250,000\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Make sure to create the finetuned model before running this cell\n",
    "tts = load_chatterbox_model(\n",
    "    \"../models/checkpoints/chatterbox_finetuned_swedish/merged_for_infer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548aa83",
   "metadata": {},
   "source": [
    "## Create, apply and save conditionals for a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da896203",
   "metadata": {},
   "source": [
    "### Create conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b419e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voice_profile_from_wav(\n",
    "    tts,\n",
    "    wav_path: str,\n",
    "    output_dir: str = \"../models/voices\",\n",
    "    name: str | None = None,\n",
    "    exaggeration: float = 0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract voice conditionals from a reference wav and save to disk.\n",
    "    Only switches to CPU when the model is on MPS (MPS cannot run prepare_conditionals).\n",
    "    \"\"\"\n",
    "    wav_path = Path(wav_path)\n",
    "    if not wav_path.exists():\n",
    "        raise FileNotFoundError(f\"Reference audio not found: {wav_path}\")\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not name or not name.strip():\n",
    "        name = wav_path.stem\n",
    "\n",
    "    # Determine original device\n",
    "    original_device = getattr(tts, \"device\", \"cpu\")\n",
    "\n",
    "    # --- Only move to CPU if using MPS ---\n",
    "    if original_device == \"mps\":\n",
    "        print(\"[voice] MPS detected â€” switching to CPU for conditionals extraction.\")\n",
    "        tts.t3.to(\"cpu\")\n",
    "        tts.s3gen.to(\"cpu\")\n",
    "        tts.ve.to(\"cpu\")\n",
    "        tts.device = \"cpu\"\n",
    "\n",
    "    # --- Extract conditionals ---\n",
    "    tts.prepare_conditionals(str(wav_path), exaggeration=exaggeration)\n",
    "    conds = tts.conds\n",
    "\n",
    "    if conds is None:\n",
    "        # restore model before raising\n",
    "        if original_device == \"mps\":\n",
    "            tts.t3.to(original_device)\n",
    "            tts.s3gen.to(original_device)\n",
    "            tts.ve.to(original_device)\n",
    "            tts.device = original_device\n",
    "        raise RuntimeError(\"prepare_conditionals() returned None â€” extraction failed.\")\n",
    "\n",
    "    # --- Save profile ---\n",
    "    out_path = output_dir / f\"{name}.pt\"\n",
    "    conds.save(out_path)\n",
    "    print(f\"[voice] Saved voice profile to: {out_path}\")\n",
    "\n",
    "    # --- Restore device if we switched ---\n",
    "    if original_device == \"mps\":\n",
    "        tts.t3.to(\"mps\")\n",
    "        tts.s3gen.to(\"mps\")\n",
    "        tts.ve.to(\"mps\")\n",
    "        tts.device = \"mps\"\n",
    "\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85acb08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Reference mel length is not equal to 2 * reference token length.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[voice] Saved voice profile to: ../models/voices/david.pt\n"
     ]
    }
   ],
   "source": [
    "voice_path = create_voice_profile_from_wav(\n",
    "    tts=tts,\n",
    "    wav_path=\"../data/david/soundfiles/david_sentence25.wav\",\n",
    "    output_dir=\"../models/voices\",\n",
    "    name=\"david\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca78078",
   "metadata": {},
   "source": [
    "### Apply conditionals to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf7d7f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_voice_profile_from_path(tts, conds_path: str):\n",
    "    \"\"\"\n",
    "    Load Conditionals (.pt) from disk and apply them to a ChatterboxMultilingualTTS model.\n",
    "    Automatically uses the device the model is already on.\n",
    "    \"\"\"\n",
    "    conds_path = Path(conds_path)\n",
    "    if not conds_path.exists():\n",
    "        raise FileNotFoundError(f\"Voice profile not found: {conds_path}\")\n",
    "\n",
    "    # Determine the model's device (fallback to CPU if somehow undefined)\n",
    "    device = getattr(tts, \"device\", \"cpu\")\n",
    "\n",
    "    # Load & move to that device\n",
    "    conds = Conditionals.load(str(conds_path), map_location=device).to(device)\n",
    "\n",
    "    # Attach to model\n",
    "    tts.conds = conds\n",
    "\n",
    "    print(f\"[voice] Applied voice profile from {conds_path} â†’ device={device}\")\n",
    "    return tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c63a95e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[voice] Applied voice profile from ../models/voices/david.pt â†’ device=cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<chatterbox.mtl_tts.ChatterboxMultilingualTTS at 0x7fc3ebb307f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_voice_profile_from_path(\n",
    "    tts,\n",
    "    \"../models/voices/david.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e2c3c",
   "metadata": {},
   "source": [
    "## Genereate soundfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c55d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    parts = re.split(r'([\\.!?])', text)\n",
    "    sents = [\"\".join(parts[i:i+2]).strip() for i in range(0, len(parts), 2)]\n",
    "    return [s for s in sents if s]\n",
    "\n",
    "def generate_chunked(tts, text, **gen_kwargs):\n",
    "    wavs = []\n",
    "    for s in split_sentences(text):\n",
    "        w = tts.generate(s, **gen_kwargs)\n",
    "        w = torch.as_tensor(w)\n",
    "        if w.ndim == 1: w = w.unsqueeze(0)\n",
    "        elif w.shape[0] > w.shape[-1]: w = w.t()\n",
    "        wavs.append(w.to(torch.float32).contiguous().clamp_(-1,1).cpu())\n",
    "    return torch.cat(wavs, dim=-1)    \n",
    "\n",
    "def synthesize_with_model(\n",
    "    tts,\n",
    "    text: str,\n",
    "    output_dir: str = \"../output\",\n",
    "    language: str = \"sv\",\n",
    "    exaggeration: float = 0.5,\n",
    "    cfg_weight: float = 0.5,\n",
    "    filename: str = \"\", # if empty, name will be timebased\n",
    "):\n",
    "    \"\"\"\n",
    "    Use an already loaded tts model to synthesize text.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Generate waveform ----\n",
    "    wav = generate_chunked(\n",
    "        tts,\n",
    "        text,\n",
    "        language_id=language,\n",
    "        exaggeration=exaggeration,\n",
    "        cfg_weight=cfg_weight,\n",
    "    )\n",
    "\n",
    "    # ---- Determine filename ----\n",
    "    if not filename: \n",
    "        timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f\"{timestamp}_{language.lower()}.wav\"\n",
    "    else:\n",
    "        # ensure extension\n",
    "        if not filename.lower().endswith(\".wav\"):\n",
    "            filename += \".wav\"\n",
    "\n",
    "    out_path = output_dir / filename\n",
    "    \n",
    "    # ---- set sample rate ----\n",
    "    sr = getattr(tts, \"sr\", getattr(tts, \"sample_rate\", 24000))\n",
    "\n",
    "    # ---- Save ----\n",
    "    ta.save(str(out_path), wav.cpu(), sr)\n",
    "\n",
    "    print(\"Saved:\", out_path)\n",
    "    return wav, sr, out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dcb252",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bbde391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Sampling:   0%|          | 0/1000 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "Sampling:  14%|â–ˆâ–        | 140/1000 [00:08<00:53, 16.16it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  14%|â–ˆâ–        | 141/1000 [00:09<00:54, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/tts-base.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4995e-09, -1.4760e-09, -4.9755e-10,  ..., -2.9102e-06,\n",
       "          -1.3889e-05, -1.6783e-06]]),\n",
       " 24000,\n",
       " PosixPath('../examples/tts-base.wav'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Text to synthesize ----\n",
    "text_sv = \"Hej, det hÃ¤r Ã¤r ett test fÃ¶r att utvÃ¤rdera vÃ¥r nya anpassade TTS-modell.\"\n",
    "\n",
    "synthesize_with_model(\n",
    "    tts=base_model,\n",
    "    text=text_sv,\n",
    "    output_dir=\"../examples\",\n",
    "    language=\"sv\",\n",
    "    exaggeration=0.5,\n",
    "    cfg_weight=0.5,\n",
    "    filename=\"tts-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60f6b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model (medium)...\n",
      "Whisper model loaded.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import whisper\n",
    "from jiwer import cer as jiwer_cer\n",
    "from jiwer import wer as jiwer_wer\n",
    "\n",
    "# Normalize_text: make it lowercase, remove punctuation and collapse multiple spaces\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^\\wÃ¥Ã¤Ã¶Ã©Ã¼ÃµÃ§Ã± ]+\", \"\", s, flags=re.UNICODE)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# CER\n",
    "def cer(ref: str, hyp: str) -> float:\n",
    "    ref_norm = normalize_text(ref)\n",
    "    hyp_norm = normalize_text(hyp)\n",
    "    return jiwer_cer(ref_norm, hyp_norm)\n",
    "\n",
    "# WER\n",
    "def wer(ref: str, hyp: str) -> float:\n",
    "    ref_norm = normalize_text(ref)\n",
    "    hyp_norm = normalize_text(hyp)\n",
    "    return jiwer_wer(ref_norm, hyp_norm)\n",
    "\n",
    "# Whisper loading\n",
    "print(\"Loading Whisper model (medium)...\")\n",
    "whisper_model = whisper.load_model(\"medium\")\n",
    "print(\"Whisper model loaded.\")\n",
    "\n",
    "def transcribe_audio(audio_path: str, language: str = \"sv\") -> str:\n",
    "    \"\"\"Transcribe TTS audio using Whisper (Swedish).\"\"\"\n",
    "    result = whisper_model.transcribe(audio_path, language=language, task=\"transcribe\")\n",
    "    return result[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07358894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 evaluation sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/nobackup/joeri765/venvs/pni-py310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:  10%|â–ˆ         | 100/1000 [00:06<00:58, 15.49it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:  10%|â–ˆ         | 101/1000 [00:06<00:55, 16.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_000.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   9%|â–‰         | 92/1000 [00:05<01:01, 14.80it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:   9%|â–‰         | 92/1000 [00:05<00:57, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_000.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   8%|â–Š         | 76/1000 [00:04<00:58, 15.93it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:   8%|â–Š         | 77/1000 [00:04<00:56, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_001.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   9%|â–‰         | 90/1000 [00:05<01:01, 14.74it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:   9%|â–‰         | 91/1000 [00:05<00:55, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_001.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   8%|â–Š         | 80/1000 [00:05<01:04, 14.28it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:   8%|â–Š         | 80/1000 [00:05<01:02, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_002.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   8%|â–Š         | 78/1000 [00:05<01:05, 14.00it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:   8%|â–Š         | 78/1000 [00:05<01:04, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_002.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   9%|â–‰         | 89/1000 [00:06<01:02, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_003.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   9%|â–‰         | 91/1000 [00:06<01:03, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_003.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:  12%|â–ˆâ–        | 118/1000 [00:08<01:02, 14.06it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:  12%|â–ˆâ–        | 118/1000 [00:08<01:01, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_004.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:  11%|â–ˆâ–        | 114/1000 [00:07<01:04, 13.67it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  12%|â–ˆâ–        | 115/1000 [00:08<01:01, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_004.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   9%|â–‰         | 92/1000 [00:06<01:03, 14.24it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 4137\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:   9%|â–‰         | 93/1000 [00:06<01:00, 15.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_005.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   9%|â–‰         | 94/1000 [00:06<01:02, 14.57it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:   9%|â–‰         | 94/1000 [00:06<00:59, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_005.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:  11%|â–ˆ         | 112/1000 [00:07<00:53, 16.67it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:  11%|â–ˆ         | 112/1000 [00:07<00:59, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_006.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:  10%|â–ˆ         | 102/1000 [00:06<00:57, 15.68it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  10%|â–ˆ         | 102/1000 [00:06<01:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_006.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   9%|â–‰         | 90/1000 [00:06<01:04, 14.21it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:   9%|â–‰         | 90/1000 [00:06<01:02, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_007.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   9%|â–‰         | 88/1000 [00:05<01:05, 14.00it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:   9%|â–‰         | 89/1000 [00:06<01:02, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_007.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   9%|â–Š         | 86/1000 [00:05<00:55, 16.47it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:   9%|â–Š         | 86/1000 [00:05<00:58, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_008.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   8%|â–Š         | 76/1000 [00:05<00:58, 15.88it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:   8%|â–Š         | 77/1000 [00:05<01:03, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_008.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:  20%|â–ˆâ–ˆ        | 204/1000 [00:14<01:01, 13.02it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ðŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:  20%|â–ˆâ–ˆ        | 205/1000 [00:14<00:55, 14.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/finetuned/finetune_009.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:  16%|â–ˆâ–Œ        | 160/1000 [00:11<00:56, 14.89it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  16%|â–ˆâ–Œ        | 160/1000 [00:11<00:58, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../examples/eval_audio/base/base_009.wav\n",
      "\n",
      "=== CER and WER EVALUATION ===\n",
      "\n",
      "[000] -----------------------------\n",
      "REF:       Det hÃ¤r Ã¤r ett test av chatterbox TTS finetuning.\n",
      "BASE:   Det hÃ¤r Ã¤r Testar Jack the Box TTS fÃ¶r inertuning.\n",
      "FT:     Det hÃ¤r Ã¤r ett test av Chatterbox TTS fina tuning.\n",
      "CER base:  0.333\n",
      "CER ft:    0.042\n",
      "WER base:  0.667\n",
      "WER ft:    0.222\n",
      "\n",
      "[001] -----------------------------\n",
      "REF:       Idag ska vi utvÃ¤rdera svensk talsyntes.\n",
      "BASE:   Idag ska vi utvÃ¤rdera svensk talsyntes.\n",
      "FT:     Idag ska vi utvÃ¤rdera svensk talsyntes.\n",
      "CER base:  0.000\n",
      "CER ft:    0.000\n",
      "WER base:  0.000\n",
      "WER ft:    0.000\n",
      "\n",
      "[002] -----------------------------\n",
      "REF:       RÃ¶sten ska vara tydlig Ã¤ven nÃ¤r meningarna blir lÃ¤ngre.\n",
      "BASE:   RÃ¶sten ska vara tydlig Ã¤ven nÃ¤r meningarna blir lÃ¤ngre.\n",
      "FT:     HÃ¶sten ska vara tydlig Ã¤ven nÃ¤r meningen blir lÃ¤ngre.\n",
      "CER base:  0.000\n",
      "CER ft:    0.074\n",
      "WER base:  0.000\n",
      "WER ft:    0.222\n",
      "\n",
      "[003] -----------------------------\n",
      "REF:       Nu prÃ¶var vi hur modellen hanterar olika vokalljud.\n",
      "BASE:   Nu prÃ¶var vi hur modellen hanterar olika vokaljud.\n",
      "FT:     Nu prÃ¶var vi hur modellen hanterar olika vokaldjud.\n",
      "CER base:  0.020\n",
      "CER ft:    0.020\n",
      "WER base:  0.125\n",
      "WER ft:    0.125\n",
      "\n",
      "[004] -----------------------------\n",
      "REF:       Sju sjuka sjÃ¶mÃ¤n skÃ¶ttes av sju skÃ¶na sjukskÃ¶terskor.\n",
      "BASE:   Sju sjuka frÃ¤mmen skÃ¶ttes av sju skÃ¶na sjukskÃ¶terskor.\n",
      "FT:     Sju sjuka skÃ¤men skÃ¶ttes av sju skÃ¶na sjukskÃ¶terskor.\n",
      "CER base:  0.096\n",
      "CER ft:    0.058\n",
      "WER base:  0.125\n",
      "WER ft:    0.125\n",
      "\n",
      "[005] -----------------------------\n",
      "REF:       Kan modellen uttala bÃ¥de korta och lÃ¥nga vokaler korrekt?\n",
      "BASE:   Kan modellen uttala bÃ¥de korta och lÃ¶ngevokaler korrekt?\n",
      "FT:     Kan modellen uttala bÃ¥de korta och lÃ¥nga vokaler korrekt?\n",
      "CER base:  0.054\n",
      "CER ft:    0.000\n",
      "WER base:  0.222\n",
      "WER ft:    0.000\n",
      "\n",
      "[006] -----------------------------\n",
      "REF:       HÃ¤r kommer en mening med siffror: tre, tolv och tjugotre.\n",
      "BASE:   HÃ¤r kommer jag mening med siffror 3, 12 och 23.\n",
      "FT:     HÃ¤r kommer en mening med siffror 3, 12 och 23.\n",
      "CER base:  0.333\n",
      "CER ft:    0.278\n",
      "WER base:  0.400\n",
      "WER ft:    0.300\n",
      "\n",
      "[007] -----------------------------\n",
      "REF:       Hur lÃ¥ter prosodin nÃ¤r tempot Ã¶kar lite grann?\n",
      "BASE:   Hur lagt det plÃ¶tsligt in nÃ¤r tempot Ã¶kar liteglad?\n",
      "FT:     Hur lÃ¥ter Poseidon nÃ¤r tempot Ã¶kar lite grann?\n",
      "CER base:  0.378\n",
      "CER ft:    0.089\n",
      "WER base:  0.750\n",
      "WER ft:    0.125\n",
      "\n",
      "[008] -----------------------------\n",
      "REF:       Jag vill hÃ¶ra om uttalet fÃ¶rÃ¤ndras nÃ¤r frÃ¥getecken anvÃ¤nds?\n",
      "BASE:   Jag vill hÃ¶ra om uttalet fÃ¶rÃ¤ndras nÃ¤r frÃ¥getecken anvÃ¤nds.\n",
      "FT:     Jag vill hÃ¶ra om uttalet fÃ¶rÃ¤ndras nÃ¤r frÃ¥getecken anvÃ¤nds.\n",
      "CER base:  0.000\n",
      "CER ft:    0.000\n",
      "WER base:  0.000\n",
      "WER ft:    0.000\n",
      "\n",
      "[009] -----------------------------\n",
      "REF:       Till sist testar vi en riktigt lÃ¥ng mening fÃ¶r att se om modellen klarar att behÃ¥lla ett naturligt flyt och en stabil volym genom hela uttalet.\n",
      "BASE:   Till sist testar vi en riktigt lÃ¥ng mening fÃ¶r att se om modellen klarar att behÃ¶lla ett naturligt frit och en stabil volym genom hela uttalet.\n",
      "FT:     Till sist testar vi en riktigt lÃ¥ng mening fÃ¶r att se om modellen klarar att behÃ¶lla ett naturligt flyt och en stabil volym genom hela uttalet.\n",
      "CER base:  0.021\n",
      "CER ft:    0.007\n",
      "WER base:  0.077\n",
      "WER ft:    0.038\n",
      "\n",
      "===== CER and WER SUMMARY =====\n",
      "Base CER: 0.124\n",
      "Finetuned CER: 0.057\n",
      "Base WER: 0.237\n",
      "Finetuned WER: 0.116\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "eval_sentences = [\n",
    "    \"Det hÃ¤r Ã¤r ett test av chatterbox TTS finetuning.\",\n",
    "    \"Idag ska vi utvÃ¤rdera svensk talsyntes.\",\n",
    "    \"RÃ¶sten ska vara tydlig Ã¤ven nÃ¤r meningarna blir lÃ¤ngre.\",\n",
    "    \"Nu prÃ¶var vi hur modellen hanterar olika vokalljud.\",\n",
    "    \"Sju sjuka sjÃ¶mÃ¤n skÃ¶ttes av sju skÃ¶na sjukskÃ¶terskor.\",\n",
    "    \"Kan modellen uttala bÃ¥de korta och lÃ¥nga vokaler korrekt?\",\n",
    "    \"HÃ¤r kommer en mening med siffror: tre, tolv och tjugotre.\",\n",
    "    \"Hur lÃ¥ter prosodin nÃ¤r tempot Ã¶kar lite grann?\",\n",
    "    \"Jag vill hÃ¶ra om uttalet fÃ¶rÃ¤ndras nÃ¤r frÃ¥getecken anvÃ¤nds?\",\n",
    "    \"Till sist testar vi en riktigt lÃ¥ng mening fÃ¶r att se om modellen klarar att behÃ¥lla ett naturligt flyt och en stabil volym genom hela uttalet.\"\n",
    "]\n",
    "\n",
    "print(\"Using\", len(eval_sentences), \"evaluation sentences.\")\n",
    "\n",
    "# Output directories\n",
    "ft_dir   = Path(\"../examples/eval_audio/finetuned\")\n",
    "base_dir = Path(\"../examples/eval_audio/base\")\n",
    "\n",
    "ft_dir.mkdir(parents=True, exist_ok=True)\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Synthesize audio \n",
    "for i, text in enumerate(eval_sentences):\n",
    "\n",
    "    # Finetuned\n",
    "    finetune_filename = f\"finetune_{i:03d}\"\n",
    "    synthesize_with_model(\n",
    "        tts=tts,\n",
    "        text=text,\n",
    "        output_dir=str(ft_dir),\n",
    "        language=\"sv\",\n",
    "        exaggeration=0.5,\n",
    "        cfg_weight=0.5,\n",
    "        filename=finetune_filename,\n",
    "    )\n",
    "\n",
    "    # Baseline\n",
    "    base_filename = f\"base_{i:03d}\"\n",
    "    synthesize_with_model(\n",
    "        tts=base_model,\n",
    "        text=text,\n",
    "        output_dir=str(base_dir),\n",
    "        language=\"sv\",\n",
    "        exaggeration=0.5,\n",
    "        cfg_weight=0.5,\n",
    "        filename=base_filename,\n",
    "    )\n",
    "\n",
    "# CER evaluation \n",
    "print(\"\\n=== CER and WER EVALUATION ===\")\n",
    "\n",
    "base_scores_cer = []\n",
    "ft_scores_cer   = []\n",
    "\n",
    "base_scores_wer = []\n",
    "ft_scores_wer   = []\n",
    "\n",
    "for i, ref_text in enumerate(eval_sentences):\n",
    "    finetune_filename = f\"finetune_{i:03d}.wav\"\n",
    "    base_filename = f\"base_{i:03d}.wav\"\n",
    "    base_path = base_dir / base_filename\n",
    "    ft_path   = ft_dir   / finetune_filename\n",
    "\n",
    "    base_transcribed = transcribe_audio(str(base_path))\n",
    "    ft_transcribed   = transcribe_audio(str(ft_path))\n",
    "\n",
    "    base_cer = cer(ref_text, base_transcribed)\n",
    "    ft_cer   = cer(ref_text, ft_transcribed)\n",
    "    \n",
    "    base_wer = wer(ref_text, base_transcribed)\n",
    "    ft_wer   = wer(ref_text, ft_transcribed)\n",
    "\n",
    "    base_scores_cer.append(base_cer)\n",
    "    ft_scores_cer.append(ft_cer)\n",
    "    \n",
    "    base_scores_wer.append(base_wer)\n",
    "    ft_scores_wer.append(ft_wer)\n",
    "\n",
    "    print(f\"\\n[{i:03d}] -----------------------------\")\n",
    "    print(f\"REF:       {ref_text}\")\n",
    "    print(f\"BASE:  {base_transcribed}\")\n",
    "    print(f\"FT:    {ft_transcribed}\")\n",
    "    print(f\"CER base:  {base_cer:.3f}\")\n",
    "    print(f\"CER ft:    {ft_cer:.3f}\")\n",
    "    print(f\"WER base:  {base_wer:.3f}\")\n",
    "    print(f\"WER ft:    {ft_wer:.3f}\")\n",
    "\n",
    "\n",
    "# Summary\n",
    "base_cer_avg = float(np.mean(base_scores_cer))\n",
    "ft_cer_vg   = float(np.mean(ft_scores_cer))\n",
    "\n",
    "base_wer_avg = float(np.mean(base_scores_wer))\n",
    "ft_wer_avg   = float(np.mean(ft_scores_wer))\n",
    "\n",
    "print(\"\\n===== CER and WER SUMMARY =====\")\n",
    "print(f\"Base CER: {base_cer_avg:.3f}\")\n",
    "print(f\"Finetuned CER: {ft_cer_vg:.3f}\")\n",
    "print(f\"Base WER: {base_wer_avg:.3f}\")\n",
    "print(f\"Finetuned WER: {ft_wer_avg:.3f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
