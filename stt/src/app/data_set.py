from sklearn.model_selection import train_test_split
from datasets import DatasetDict, Audio, load_dataset, concatenate_datasets
from pathlib import Path
import numpy as np
from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift

class DataSet:
    def __init__(self, 
                 manifest_path: str,
                 recordings_root: str, 
                 user_name: str, 
                 model_setup,
                 seed: int = 42,
                 split_ratios: tuple = (0.6, 0.2, 0.2),
                 use_data_augmentation: bool = False): 
        
        self.model_setup = model_setup
        self.processor = model_setup.processor
        self.tokenizer = model_setup.tokenizer
        self.user_name = user_name
        self.seed = seed
        self.split_ratios = split_ratios
        self.use_data_augmentation = use_data_augmentation 
        
        # Define the Augmentation Pipeline
        if self.use_data_augmentation:
            self.augmentor = Compose([
                AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.010, p=0.3),
                TimeStretch(min_rate=0.9, max_rate=1.1, p=0.3),
                PitchShift(min_semitones=-2, max_semitones=2, p=0.3),
                Shift(min_fraction=-0.3, max_fraction=0.3, p=0.3),
            ])

        self.manifest_path = Path(manifest_path).resolve()
        self.recordings_root = Path(recordings_root)
        self.ds_dict = self._load_and_prepare_dataset()

    def augment_audio_batch_func(self, batch):
        augmented_audio = []
        for x in batch["sentence"]:
            original_array = x["array"].astype(np.float32) 
            sr = x["sampling_rate"]
            
            aug_array = self.augmentor(samples=original_array, sample_rate=sr)
            augmented_audio.append({"array": aug_array, "sampling_rate": sr})
            
        batch["sentence"] = augmented_audio
        return batch
        
    def formatting_prompts_func(self, batch):
        audio_arrays = [x["array"] for x in batch["sentence"]]
        sampling_rate = batch["sentence"][0]["sampling_rate"]
        
        features = self.processor.feature_extractor(
            audio_arrays, 
            sampling_rate=sampling_rate
        )
        
        tokenized_text = self.tokenizer(batch["text"])
        
        return {
            "input_features": features.input_features,
            "labels": tokenized_text.input_ids,
        }


    def indices_to_paths(self, batch):
        out = []
        filenames = []
        for a in batch["sentence"]:
            if isinstance(a, str) and not a.strip().isdigit():
                # Fix for relative paths generated by backend with DATA_DIR="./data"
                # If path starts with data/ or ./data/, map it to /app/data/
                if a.startswith("data/") or a.startswith("./data/"):
                    clean_path = a.lstrip("./")
                    abs_path = f"/app/{clean_path}"
                    out.append(abs_path)
                    filenames.append(Path(abs_path).name)
                else:
                    out.append(a)
                    filenames.append(Path(a).name)
                continue
            idx = int(a) if isinstance(a, (int, float)) else int(a.strip())
            rel = Path(self.recordings_root / f"{self.user_name}_sentence{idx}.wav")
            out.append(str(rel))
            filenames.append(rel.name)
        batch["sentence"] = out
        batch["filename"] = filenames
        return batch

    def _load_and_prepare_dataset(self) -> DatasetDict:
        dataset = load_dataset("json", data_files={"train": str(self.manifest_path)}, encoding="utf-8")["train"]
        dataset = dataset.map(self.indices_to_paths, desc="Mapping paths", batched=True)
        dataset = dataset.cast_column("sentence", Audio(sampling_rate=16000))

        train_ratio, val_ratio, test_ratio = self.split_ratios
        total = train_ratio + val_ratio + test_ratio
        
        val_ratio = val_ratio / total
        test_ratio = test_ratio / total
        
        splits = dataset.train_test_split(test_size=test_ratio, seed=self.seed)
        test_dataset = splits["test"]
        remaining = splits["train"]

        val_relative_size = val_ratio / (1 - test_ratio)
        
        train_val = remaining.train_test_split(test_size=val_relative_size, seed=self.seed)
        train_dataset = train_val["train"]
        val_dataset = train_val["test"]

        # DATA AUGMENTATION
        if self.use_data_augmentation:
            print(f"Applying data augmentation to {len(train_dataset)} examples...")
            
            augmented_dataset = train_dataset.map(
                self.augment_audio_batch_func,
                desc="Augmenting audio",
                batched=True,
                batch_size=50,
                num_proc=1 
            )
            
            # Combine Original + Augmented
            train_dataset = concatenate_datasets([train_dataset, augmented_dataset])
            print(f"Augmentation complete. New training size: {len(train_dataset)}")

        dataset_dict = DatasetDict({
            "train": train_dataset,
            "validation": val_dataset,
            "test": test_dataset,
        })
        
        print(f"Final Split Sizes -> Train: {len(dataset_dict['train'])}, Val: {len(dataset_dict['validation'])}, Test: {len(dataset_dict['test'])}")
        
        dataset_dict = dataset_dict.map(
            self.formatting_prompts_func, 
            desc="Formatting prompts",
            batched=True,    
            batch_size=50,   
            num_proc=1,
            remove_columns=["text", "filename", "sentence"] 
        )

        return dataset_dict